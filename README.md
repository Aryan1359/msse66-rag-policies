# MSSE66 RAG â€” Company Policies Q&A

Retrievalâ€‘Augmented Generation (RAG) app that answers questions about a small corpus of **company policies**. Built as part of the **MSSE66+ AI Engineering Project**, aligned to the rubric (environment, CI, ingestion, retrieval, embeddings, evaluation).

---

## ğŸ“Œ Status (end of Phaseâ€¯3)

âœ… Repoâ€¯+â€¯CIâ€¯green
âœ… Keyword retrieval working (`/search?mode=keyword`)
âœ… Local sentenceâ€‘transformer embeddings created (`allâ€‘MiniLMâ€‘L6â€‘v2`)
âœ… Vector retrieval working (`/search?mode=vector`)
âœ… Consistent JSON responses (`mode`, `query`, `topk`, `results`, `doc_id`, `chunk_id`, `score`, `preview`)
âœ… Nextâ€¯stepâ€¯â†’â€¯addâ€¯`sources`â€¯arrayâ€¯+â€¯miniâ€¯evaluationâ€¯dataset

---

## ğŸš€ Quickstart (GitHubâ€¯Codespaces)

```bash
#â€¯1)â€¯Activateâ€¯virtualâ€¯env
sourceâ€¯.venv/bin/activate

#â€¯2)â€¯(Re)buildâ€¯JSONLâ€¯indexâ€¯ifâ€¯policiesâ€¯changed
pythonâ€¯scripts/index_jsonl.py

#â€¯3)â€¯Generateâ€¯embeddingsâ€¯(ifâ€¯notâ€¯yetâ€¯done)
pythonâ€¯scripts/embed_index.py

#â€¯4)â€¯Runâ€¯theâ€¯Flaskâ€¯appâ€¯(portâ€¯8000)
pythonâ€¯app.py
```

**Testâ€¯endpoints:**

```bash
#â€¯Healthâ€¯check
curlâ€¯"http://127.0.0.1:8000/health"

#â€¯Keywordâ€¯searchâ€¯(default)
curlâ€¯"http://127.0.0.1:8000/search?q=pto%20accrual&topk=3"

#â€¯Vectorâ€¯search
curlâ€¯"http://127.0.0.1:8000/search?q=pto%20accrual%20policy&mode=vector&topk=3"
```

> â€¯Ifâ€¯youâ€™reâ€¯usingâ€¯Codespacesâ€¯browserâ€¯preview,â€¯useâ€¯yourâ€¯forwardedâ€¯URLâ€¯likeâ€¯`https://<id>-8000.app.github.dev/`.

---

## ğŸ“â€¯Repositoryâ€¯Structure

```
msse66â€‘ragâ€‘policies/
â”œâ”€â€¯app.pyâ€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯#â€¯Flaskâ€¯appâ€¯(/,â€¯/health,â€¯/search)
â”œâ”€â€¯data/
â”‚â€¯â€¯â”œâ”€â€¯policies/â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯#â€¯Markdownâ€¯policyâ€¯documents
â”‚â€¯â€¯â”‚â€¯â€¯â”œâ”€â€¯01â€‘pto.md
â”‚â€¯â€¯â”‚â€¯â€¯â”œâ”€â€¯02â€‘expenses.md
â”‚â€¯â€¯â”‚â€¯â€¯â””â”€â€¯03â€‘remoteâ€‘work.md
â”‚â€¯â€¯â””â”€â€¯index/
â”‚â€¯â€¯â€¯â€¯â”œâ”€â€¯policies.jsonlâ€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯#â€¯Chunkedâ€¯index
â”‚â€¯â€¯â€¯â€¯â”œâ”€â€¯policies.npyâ€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯#â€¯Embeddingâ€¯matrix
â”‚â€¯â€¯â€¯â€¯â””â”€â€¯meta.jsonâ€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯#â€¯Modelâ€¯metadataâ€¯+â€¯idâ€¯map
â”œâ”€â€¯scripts/
â”‚â€¯â€¯â”œâ”€â€¯ingest.pyâ€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯#â€¯Documentâ€¯stats
â”‚â€¯â€¯â”œâ”€â€¯chunk.pyâ€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯#â€¯Overlappingâ€¯chunker
â”‚â€¯â€¯â”œâ”€â€¯index_jsonl.pyâ€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯#â€¯Buildâ€¯index
â”‚â€¯â€¯â”œâ”€â€¯embed_index.pyâ€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯#â€¯Encodeâ€¯chunksâ€¯â†’â€¯vectors
â”‚â€¯â€¯â””â”€â€¯vector_search.pyâ€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯#â€¯NumPyâ€¯cosineâ€¯retriever
â”œâ”€â€¯.github/workflows/ci.ymlâ€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯#â€¯CIâ€¯smokeâ€¯test
â”œâ”€â€¯requirements.txt
â”œâ”€â€¯Instruction.md
â”œâ”€â€¯LEARNINGâ€‘GUIDE.md
â”œâ”€â€¯checklist.md
â””â”€â€¯PROGRESSâ€‘LOG.md
```

---

## ğŸ§©â€¯Implementedâ€¯Endpoints

###â€¯GETâ€¯`/health`
â†’â€¯`{"status":"ok"}`

###â€¯GETâ€¯`/search`

| â€¯Paramâ€¯  | â€¯Typeâ€¯   | â€¯Defaultâ€¯   | â€¯Descriptionâ€¯                         |
| :------- | :------- | :---------- | :------------------------------------ |
| â€¯`q`â€¯    | â€¯stringâ€¯ | â€¯â€”â€¯         | â€¯Queryâ€¯textâ€¯                          |
| â€¯`topk`â€¯ | â€¯intâ€¯    | â€¯3â€¯         | â€¯Topâ€‘kâ€¯resultsâ€¯                       |
| â€¯`mode`â€¯ | â€¯stringâ€¯ | â€¯`keyword`â€¯ | â€¯Searchâ€¯typeâ€¯(`keyword`â€¯orâ€¯`vector`)â€¯ |

**Exampleâ€¯(keyword):**

```bash
curlâ€¯"http://127.0.0.1:8000/search?q=vacation%20policy&topk=3"
```

Response:

```json
{
â€¯"mode":â€¯"keyword",
â€¯"query":â€¯"vacationâ€¯policy",
â€¯"topk":â€¯3,
â€¯"results":â€¯[
â€¯â€¯{"doc_id":â€¯"01â€‘pto",â€¯"chunk_id":â€¯1,â€¯"score":â€¯5.0,â€¯"preview":â€¯"#â€¯Paidâ€¯Timeâ€¯Offâ€¦"}
â€¯]
}
```

**Exampleâ€¯(vector):**

```bash
curlâ€¯"http://127.0.0.1:8000/search?q=pto%20accrual%20policy&mode=vector&topk=3"
```

Response:

```json
{
â€¯"mode":â€¯"vector",
â€¯"query":â€¯"ptoâ€¯accrualâ€¯policy",
â€¯"topk":â€¯3,
â€¯"results":â€¯[
â€¯â€¯{"doc_id":â€¯"01â€‘pto",â€¯"chunk_id":â€¯1,â€¯"score":â€¯0.65,â€¯"preview":â€¯"#â€¯Paidâ€¯Timeâ€¯Offâ€¦"},
â€¯â€¯{"doc_id":â€¯"02â€‘expenses",â€¯"chunk_id":â€¯1,â€¯"score":â€¯0.42,â€¯"preview":â€¯"#â€¯Employeeâ€¯Expenseâ€¦"}
â€¯]
}
```

---

## ğŸ§ â€¯Phaseâ€¯Summary

| â€¯Phaseâ€¯ | â€¯Goalâ€¯                          | â€¯Keyâ€¯Outputsâ€¯                                        |
| :------ | :------------------------------ | :--------------------------------------------------- |
| â€¯1â€¯     | â€¯Environmentâ€¯+â€¯Flaskâ€¯setupâ€¯     | â€¯`app.py`â€¯(/,â€¯/health)â€¯                              |
| â€¯2â€¯     | â€¯Ingestionâ€¯+â€¯keywordâ€¯retrievalâ€¯ | â€¯`policies.jsonl`,â€¯`/search?mode=keyword`â€¯           |
| â€¯3â€¯     | â€¯Embeddingsâ€¯+â€¯vectorâ€¯searchâ€¯    | â€¯`policies.npy`,â€¯`meta.json`,â€¯`/search?mode=vector`â€¯ |
| â€¯4â€¯     | â€¯Answerâ€¯synthesisâ€¯(LLM)â€¯        | â€¯pendingâ€¯                                            |
| â€¯5â€¯     | â€¯Evaluationâ€¯metricsâ€¯            | â€¯pendingâ€¯                                            |
| â€¯6â€¯     | â€¯Deploymentâ€¯(Render/Vercel)â€¯    | â€¯pendingâ€¯                                            |

---

## ğŸ§®â€¯Dependencies

```
Flask==3.0.3
pythonâ€‘dotenv==1.0.1
sentenceâ€‘transformers==2.7.0
numpy==1.26.4
```

---

## ğŸ§­â€¯Nextâ€¯Phaseâ€¯Plan

1ï¸âƒ£â€¯Addâ€¯`sources`â€¯arrayâ€¯toâ€¯eachâ€¯responseâ€¯(`[{doc_id,â€¯chunk_id}]`).
2ï¸âƒ£â€¯Keepâ€¯defaultâ€¯`mode=keyword`,â€¯documentâ€¯`mode=vector`.
3ï¸âƒ£â€¯Seedâ€¯tinyâ€¯Q&Aâ€¯evaluationâ€¯datasetâ€¯(`data/eval/`).
4ï¸âƒ£â€¯Addâ€¯CIâ€¯latencyâ€¯+â€¯groundednessâ€¯metrics.

---

## Phase 3 additions

### `/search` now includes `sources[]`
The search API returns a compact citation list for evaluation and RAG:
```json
{
  "mode": "keyword",
  "query": "pto accrual",
  "results": [ ... ],
  "topk": 3,
  "sources": [{"doc_id": "01-pto", "chunk_id": 1}]
}
```

### Tiny evaluation

Seed set (5 items): `data/eval/qa_sample.json`

Run the evaluator against the local server:

```bash
python app.py
# in another shell:
python scripts/eval.py --qa data/eval/qa_sample.json --base-url http://127.0.0.1:8000 --mode keyword --topk 3
```

Outputs per-item overlap and a latency roll-up:

```
[summary] hits=5/5 (100%)
[latency] min=..ms p50=..ms avg=..ms p95~=..ms
```

---

## Phase 4 â€“ RAG Answer Synthesis (WIP)


## Phase 4 â€“ RAG Answer Synthesis

### scripts/generate_answer.py (retrieval â†’ prompt â†’ LLM â†’ citations)

- End-to-end RAG answer generation: retrieves top policy chunks, builds a prompt, calls the LLM (if configured), and post-processes the answer.
- If LLM is not configured, falls back to extractive summary with a clear note.
- CLI usage (module mode recommended):

  ```bash
  python -m scripts.generate_answer --q "Whatâ€™s our PTO carryover limit?" --topk 3
  # Retrieval modes:
  #   RETRIEVAL_MODE=keyword (default, no embeddings needed)
  #   RETRIEVAL_MODE=http (calls /search?mode=vector)
  #   RETRIEVAL_MODE=vector (local, needs embeddings)
  ```

### /ask endpoint (POST + GET)

- Unified endpoint for RAG answer synthesis.
- Returns JSON with:
  - `question`, `answer`, `sources`, `source_labels`, `retrieval_ms`, `llm_ms`, `model`, `tokens`
- `source_labels` is a mapping `{S1: {doc_id, chunk_id}, ...}` matching the order of `sources[]`.
- **Citation format:** Answers include inline `[S1]`, `[S2]`, ... that correspond 1:1 to the returned `sources[]` and `source_labels`.

#### Quickstart

Run server:

```bash
python app.py
```

Ask a question (POST):

```bash
curl -s -X POST http://127.0.0.1:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"question":"How do holidays accrue?","topk":4}' | jq
```

Ask a question (GET):

```bash
curl -s "http://127.0.0.1:8000/ask?q=PTO%20accrual%20policy%3F&topk=3" | jq
```

CLI generator (module mode):

```bash
python -m scripts.generate_answer --q "Whatâ€™s our PTO carryover limit?" --topk 3
```

#### Retrieval modes

Set the environment variable `RETRIEVAL_MODE=keyword|http|vector` (default: `keyword`).

```bash
# default keyword (no embeddings required)
python -m scripts.generate_answer --q "PTO accrual policy?" --topk 4

# force HTTP vector (server must be running)
RETRIEVAL_MODE=http python -m scripts.generate_answer --q "PTO accrual policy?" --topk 4

# force local vector (requires embeddings available)
RETRIEVAL_MODE=vector python -m scripts.generate_answer --q "PTO accrual policy?" --topk 4
```

#### LLM configuration

- `GROQ_API_KEY` (required for LLM calls)
- `RAG_MODEL` (default: llama-3.1-8b-instruct)
- `RAG_MAX_TOKENS` (default: 512)
- If no API key is present, the system falls back to extractive summary with a note: `(LLM disabled; extractive summary)`

#### Troubleshooting

- If 404 on `/ask`: kill old process, restart with `python app.py`.
- If zero hits: ensure `data/index/policies.jsonl` exists (rebuild with `python scripts/index_jsonl.py`) and try `RETRIEVAL_MODE=keyword`.
