

# MSSE66 RAG â€” Company Policies Q&A

## ğŸŒ Live Demo
Deployed on Render (Free Tier):  
https://msse66-rag-policies.onrender.com

**Health check**
```bash
curl -sS https://msse66-rag-policies.onrender.com/health
```

---

## ğŸš€ What is this?

**A modular, production-ready Retrieval-Augmented Generation (RAG) pipeline for Q&A over company policies.**

- Stepwise, blueprint-based Flask app: each step (upload, parse, chunk, embed, search, ask, evaluate) is a separate page and module.
- Modern, responsive UI (Bootstrap, Lottie animations, sidebar navigation).
- Supports Markdown, HTML,  TXT, and PDF files.
- Embedding via MiniLM (sentence-transformers), vector search with FAISS or NumPy.
- LLM providers: OpenRouter, Groq, OpenAI (API key management in UI).
- All user questions, answers, and metrics are logged for evaluation.

---

## ğŸ› ï¸ Features

- **Stepwise pipeline:** upload â†’ parse â†’ clean â†’ chunk â†’ embed â†’ search â†’ ask (RAG) â†’ evaluate
- **Provider-agnostic:** easily switch between LLM providers
- **Metrics dashboard:** grounded rate, citation correctness, latency, recent questions
- **Sample questions/answers:** for manual evaluation (see Step 7)
- **All steps and logs are reproducible and persistent** (for Render deployment)
- **Easy deployment:** Render (Procfile, gunicorn) and local dev

---

## ğŸ—‚ï¸ Directory Structure

- `app.py`: Main Flask app, blueprint registration
- `steps/stepX/`: Each stepâ€™s blueprint, logic, and logs
- `templates/steps/step_X.html`: UI for each step
- `data/`: Uploaded files, embeddings, and indexes
- `scripts/`: Utilities for chunking, embedding, search, and evaluation
- `requirements.txt`: All dependencies (Flask, sentence-transformers, FAISS, etc)

---

## ğŸ§‘â€ğŸ’» How to Use

1. **Upload** policy files (Step 1)
2. **Parse and clean** (Steps 2â€“3)
3. **Chunk** by heading or token (Step 4)
4. **Embed** for semantic search (Step 5)
5. **Search and preview** retrieval (Step 6)
6. **Ask questions** (Step 7): select provider, chunking method, and parameters
7. **View metrics and recent questions** (Step 8)
8. **Compare your answers** to gold-standard samples (shown in Step 7)

---

## ğŸ“Š Metrics & Evaluation

- Step 8 dashboard: grounded rate, citation correctness, median latency, recent questions
- All logs are stored in `/steps/step7/logs/ask.jsonl` and `/steps/step6/logs/search.jsonl`
- Use the sample questions/answers table in Step 7 for manual or automated evaluation

---

## ğŸ—ï¸ Deployment

- One-click deploy to Render (free tier supported)
- All required files and logs are committed for reproducibility
- API keys can be managed via UI or repo secrets

---

## ğŸ¤ Contributing

Pull requests are welcome! Modular codebase, easy to extend or add new steps/providers.

---

## ğŸ“š Example API Usage

**Ask a question via API:**
```bash
curl -sS -X POST https://msse66-rag-policies.onrender.com/ask \
  -H "Content-Type: application/json" \
  -d '{"question":"What is the PTO policy?"}'
```

---

## ğŸ“ License

MIT License. See LICENSE file.
* **Ingestion & keyword retrieval** (local, simple & fast)
* **Grounded answers** with citations via `sources[]`
* **Automated evaluation & CI gate** (groundedness, citation accuracy, latency)
* **One-click deploy** on Render (Procfile + `gunicorn`)

Current `/ask` behavior: **extractive summary** (LLM disabled), with returned `sources[]` for grounding.

---

## âœ… Current Status (v0.7.1)

* Endpoints: `/`, `/health`, `/search`, `/ask`
* Keyword retrieval working end-to-end
* `/ask` returns: `answer`, `question`, `sources[]`, `source_labels`, timing fields
* **Evaluation Gate** in CI with thresholds (p95 latency + grounding/citation rates)
* Deployment scaffold merged; live service on **Render**

Tag history:
`v0.4.0` (RAG synthesis scaffold) â†’ `v0.5.0` (eval hook) â†’ `v0.6.0` (CI gate) â†’ `v0.7.0` (deploy scaffold) â†’ **`v0.7.1` (Live Demo docs)**

---

## ğŸš€ Quickstart (Local / Codespaces)

```bash
# 1) Activate virtual env
source .venv/bin/activate

# 2) (Re)build the lightweight keyword index
python scripts/index_jsonl.py

# 3) Run the Flask app on port 8000
python app.py
```

**Test endpoints (local):**

```bash
# Health
curl "http://127.0.0.1:8000/health"

# Keyword search
curl "http://127.0.0.1:8000/search?q=pto%20policy&topk=3"

# Ask (POST)
curl -s -X POST http://127.0.0.1:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"question":"What is the PTO policy?"}'
```

> Using Codespaces preview? Open the forwarded URL like `https://<id>-8000.app.github.dev/`.

---

## ğŸ§© API

### GET `/health`

Returns `{"status":"ok"}`.

### GET `/search`

Query string parameters:

| Param  | Type   | Default   | Description                             |
| :----- | :----- | :-------- | :-------------------------------------- |
| `q`    | string | â€”         | Query text                              |
| `topk` | int    | `3`       | Number of results                       |
| `mode` | string | `keyword` | Retrieval mode (`keyword` or `vector`)* |

> *Vector mode is scaffolded; keyword is the default and used in production today.

**Response shape (example):**

```json
{
  "mode": "keyword",
  "query": "pto policy",
  "topk": 3,
  "results": [
    {"doc_id":"01-pto","chunk_id":1,"score":5.0,"preview":"# Paid Time Off â€¦"}
  ],
  "sources": [{"doc_id":"01-pto","chunk_id":1}]
}
```

### POST `/ask`

Body:

```json
{"question": "<your question>", "topk": 4}
```

**Response shape (example):**

```json
{
  "answer": "â€¦extractive summaryâ€¦",
  "question": "What is the PTO policy?",
  "retrieval_ms": 18,
  "llm_ms": 0,
  "model": "disabled",
  "source_labels": {"S1":{"doc_id":"01-pto","chunk_id":1}},
  "sources": [
    {"doc_id":"01-pto","chunk_id":1,"score":0.0}
  ],
  "tokens": 0
}
```

---

## ğŸ“Š Evaluation & CI Gate

Automated evaluation for `/ask` lives in **`scripts/eval_ask.py`** and is wired into CI (**`.github/workflows/eval.yml`**).

**Metrics**

* **Groundedness**: % of answers returning a non-empty `sources[]`.
* **Citation Accuracy**: (heuristic) at least one cited source matches returned sources.
* **Latency**: end-to-end time; we report p50 and p95 (ms) without NumPy.

**Thresholds (defaults)**

* `--min-grounded 0.75`
* `--min-citation 0.75`
* `--p95-total 4000` (ms)

The gate **fails** (exit code `2`) if any threshold is missed.

**Run locally**

```bash
# quick sample
python scripts/eval_ask.py --limit 5

# full gate with defaults
make eval-gate

# override thresholds
MIN_GROUNDED=0.8 MIN_CITATION=0.8 P95_MS=3500 make eval-gate
```

**Artifacts** (written locally and uploaded in CI)

* `data/eval/latest_metrics.json` â€” machine-readable summary + per-item results
* `data/eval/latest_metrics.md` â€” human-readable report

---

## ğŸ“ Repository Structure


```
msse66-rag-policies/
â”œâ”€ app.py                      # Main Flask app, blueprint registration
â”œâ”€ requirements.txt            # All Python dependencies
â”œâ”€ runtime.txt                 # Python version for Render
â”œâ”€ LICENSE
â”œâ”€ PROGRESS-LOG.md
â”œâ”€ README.md
â”œâ”€ data/
â”‚  â”œâ”€ policies/                # Uploaded policy docs (md, txt, pdf, etc.)
â”‚  â””â”€ index/                   # Built indexes (keyword, vector, meta)
â”‚      â”œâ”€ meta.json
â”‚      â”œâ”€ policies.jsonl
â”‚      â””â”€ policies.npy
â”œâ”€ scripts/                    # Utilities for chunking, embedding, search, eval
â”‚  â”œâ”€ chunk.py
â”‚  â”œâ”€ embed_index.py
â”‚  â”œâ”€ index_jsonl.py
â”‚  â”œâ”€ ingest.py
â”‚  â”œâ”€ search_jsonl.py
â”‚  â”œâ”€ vector_search.py
â”‚  â””â”€ ... (eval, test, utils)
â”œâ”€ steps/                      # Modular blueprints for each pipeline step
â”‚  â”œâ”€ step1/ ... step8/        # Each step: routes, logic, logs
â”‚  â”‚   â”œâ”€ stepX_routes.py
â”‚  â”‚   â”œâ”€ services_*.py
â”‚  â”‚   â””â”€ logs/
â”‚  â””â”€ ...
â”œâ”€ templates/
â”‚  â”œâ”€ base.html                # Main layout, sidebar, footer
â”‚  â”œâ”€ index.html               # Homepage (with animation)
â”‚  â””â”€ steps/step_X.html        # UI for each step (1â€“8)
â”œâ”€ .github/workflows/eval.yml  # Evaluation workflow (CI)
â”œâ”€ Makefile                    # `make eval-gate` for CI
â”œâ”€ Procfile                    # Production entrypoint (gunicorn)
â”œâ”€ render/render.yaml          # Render service config (free plan)
â””â”€ ... (other docs, guides)
```

---

## ğŸš€ Deployment (Render Free Tier)

Included:

* `Procfile` â†’ `gunicorn app:app --bind 0.0.0.0:${PORT} --timeout 120`
* `render/render.yaml` (free plan)
* `requirements.txt` includes `gunicorn`

One-time setup:

1. In Render: **New â†’ Web Service â†’ select repo**
2. Build command: `pip install -r requirements.txt`
3. Start command: `gunicorn app:app --bind 0.0.0.0:${PORT} --timeout 120`
4. Health check path: `/health`
5. Env vars: `PYTHON_VERSION=3.12.1`, `FLASK_ENV=production`

**Local prod-like run**

```bash
pip install -r requirements.txt
PORT=8000 gunicorn app:app --bind 0.0.0.0:$PORT --timeout 120
# open http://127.0.0.1:8000/health
```

---

## ğŸ‘¤ Maintainer

* Aryan Yaghobi â€” [https://github.com/Aryan1359](https://github.com/Aryan1359)

````
