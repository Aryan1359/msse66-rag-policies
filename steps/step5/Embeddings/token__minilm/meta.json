[
  {
    "doc_id": "etops_5503_50605121425",
    "chunk_id": 0,
    "source_file": "etops_5503_50605121425.jsonl",
    "method": "token",
    "params": {
      "window_size": 700,
      "overlap": 150,
      "tokenization": "whitespace_approx"
    },
    "source_hash": "6d134ef09d90310e823643ea613c83ca0028f182cffae23e19480ced0591d343",
    "text": "Boeing Pilots Flight Training Bulletin Revision Cycle 2 Updates Flight Ops Division ‚îÇ 6/5/25 For supplemental information please review the following RC2 Updates: ETOPS 1. Preflight Procedures ETOPS 2. Enroute Procedures Before Start - CA RC2 ‚Äì Training Highlights Podcast You can also find these videos and podcast on Alaska Pilot Training App. Thank you, Your Flight Training Team Page | 1",
    "vector_index": 0
  },
  {
    "doc_id": "hydropower_formula_live_demo_html",
    "chunk_id": 0,
    "source_file": "hydropower_formula_live_demo_html.jsonl",
    "method": "token",
    "params": {
      "window_size": 700,
      "overlap": 150,
      "tokenization": "whitespace_approx"
    },
    "source_hash": "c4184bd9f49e431d03a3f3808415778e3f330c562e3090f3698ffa23f3971f82",
    "text": "Hydropower Formula ‚Äì Live Demo Hydropower Formula ‚Äì Live Demo Interactive calculator for P = Œ∑ ¬∑ œÅ ¬∑ g ¬∑ Q ¬∑ h (all SI units). Adjust inputs and see power update instantly. Inputs Efficiency Œ∑ Flow rate Q (m3/s) Head h (m) Advanced (density œÅ and gravity g) Density œÅ (kg/m3) Gravity g (m/s2) Reset Three Gorges example Units: SI (W, m, m3/s) Output ‚Äî MW P = Œ∑¬∑œÅ¬∑g¬∑Q¬∑h = ‚Äî W Scale reference: bar fills at 25 GW Per‚Äêm3 energy and intuition Quantity Expression Value Energy per m3 of water (at head h) E/m3 = œÅ¬∑g¬∑h ‚Äî J/m3 Power per (m3/s) of flow P/(m3/s) = Œ∑¬∑œÅ¬∑g¬∑h ‚Äî W per (m3/s) Show full breakdown Œ∑ (efficiency) ‚Äî œÅ (kg/m3) ‚Äî g (m/s2) ‚Äî Q (m3/s) ‚Äî h (m) ‚Äî Product Œ∑¬∑œÅ¬∑g¬∑Q¬∑h ‚Äî Nearest MW: ‚Äî Also: ‚Äî GW How to talk about it in your presentation P (power) grows when either the drop is taller ( h increases) or more water moves each second ( Q increases). The constant terms œÅ (water density) and g (gravity) turn height into energy, and Œ∑ captures real‚Äêworld losses in the turbine/generator. Together: P = Œ∑¬∑œÅ¬∑g¬∑Q¬∑h . Use the sliders live while presenting to show how doubling Q doubles power, or how raising h boosts power linearly. Open ‚ÄúAdvanced‚Äù to explain why we typically fix œÅ‚âà1000 kg/m3 and g‚âà9.81 m/s2. Quote the ‚ÄúPower per (m3/s)‚Äù number to make quick mental math: each extra m3/s of flow adds that many Watts at your chosen head and Œ∑.",
    "vector_index": 1
  },
  {
    "doc_id": "readme",
    "chunk_id": 0,
    "source_file": "readme.jsonl",
    "method": "token",
    "params": {
      "window_size": 700,
      "overlap": 150,
      "tokenization": "whitespace_approx"
    },
    "source_hash": "ded7e1292b1c3132e57d9bc201765cec81a4fe4801703ce8116d37917026c58e",
    "text": "# MSSE66 RAG ‚Äî Company Policies Q&A Retrieval‚ÄêAugmented Generation (RAG) app that answers questions about a small corpus of **company policies**. Built as part of the **MSSE66+ AI Engineering Project** and aligned to the rubric (env + CI, ingestion, retrieval, deploy, evaluation). --- ## üìå Status (end of Phase 2) **Done:** * Repo + Codespaces, `.venv`, Python pinned (3.12.1 via `.python-version`, `runtime.txt`) * Minimal deps: `Flask`, `python-dotenv` * Flask endpoints: `/` and `/health` * CI: `.github/workflows/ci.yml` smoke‚Äêtests `import app` * Corpus: `data/policies/*.md` (PTO, Expenses, Remote Work) * Ingestion & Indexing scripts: * `scripts/ingest.py` ‚Üí doc stats * `scripts/chunk.py` ‚Üí overlapping chunks * `scripts/index_jsonl.py` ‚Üí writes `data/index/policies.jsonl` * `scripts/search_jsonl.py` ‚Üí tiny keyword search (CLI) * API: `/search?q=...&topk=...` returns keyword‚Äêmatched chunks (JSON) **Next:** Phase 3 (Embeddings + vector search) ‚Üí Phase 4 (UI) ‚Üí Phase 5 (Deploy) ‚Üí Phase 6 (Eval) --- ## üöÄ Quickstart (GitHub Codespaces) ```bash # 1) Activate the virtualenv source .venv/bin/activate # 2) (Re)build JSONL index if you changed policies python scripts/index_jsonl.py # 3) Run the app (serves on port 8000) python app.py ``` **Test locally from the terminal inside Codespaces:** ```bash # Health curl \"http://127.0.0.1:8000/health\" # Keyword search curl \"http://127.0.0.1:8000/search?q=pto%20accrual&topk=3\" ``` > In the browser, use your **forwarded URL** (looks like `https://<id>-8000.app.github.dev/`). In the terminal, prefer `http://127.0.0.1:8000`. --- ## üß© What‚Äôs implemented (Phase 2) * **Corpus** in `data/policies/`: small, legal‚Äêto‚Äêuse Markdown files. * **Loader (`scripts/ingest.py`)** prints words/lines/headings per file. * **Chunker (`scripts/chunk.py`)** creates ~600‚Äêchar chunks with ~100‚Äêchar overlap, preferring breaks on blank lines/headings. * **Index writer (`scripts/index_jsonl.py`)** emits one JSON object per chunk to `data/index/policies.jsonl` with ids, text, and rough token counts. * **Keyword search (CLI)** scores by simple term frequencies with word boundaries. * **Flask `/search`** mirrors the CLI search and returns top‚Äêk results with previews and metadata. --- ## üìÅ Repository Structure ``` msse66-rag-policies/ ‚îú‚îÄ app.py # Flask app with /, /health, /search ‚îú‚îÄ data/ ‚îÇ ‚îú‚îÄ policies/ # Policy corpus (Markdown) ‚îÇ ‚îÇ ‚îú‚îÄ 01-pto.md ‚îÇ ‚îÇ ‚îú‚îÄ 02-expenses.md ‚îÇ ‚îÇ ‚îî‚îÄ 03-remote-work.md ‚îÇ ‚îî‚îÄ index/ ‚îÇ ‚îî‚îÄ policies.jsonl # Generated JSONL index (build via scripts) ‚îú‚îÄ scripts/ ‚îÇ ‚îú‚îÄ ingest.py # Corpus stats ‚îÇ ‚îú‚îÄ chunk.py # Overlapping chunker ‚îÇ ‚îú‚îÄ index_jsonl.py # Write JSONL index ‚îÇ ‚îî‚îÄ search_jsonl.py # CLI keyword search over JSONL ‚îú‚îÄ .github/workflows/ci.yml # CI: install deps + smoke test ‚îú‚îÄ requirements.txt ‚îú‚îÄ .python-version ‚îú‚îÄ runtime.txt ‚îú‚îÄ Instruction.md # Mentor/working-mode instructions ‚îú‚îÄ LEARNING-GUIDE.md # Beginner guide (what/why/how) ‚îú‚îÄ checklist.md # Master checklist (rubric aligned) ‚îî‚îÄ PROGRESS-LOG.md # Chronological log ``` --- ## üîÑ Branch / PR Workflow (beginner‚Äêproof) 1. Create a feature branch: `git checkout -b <feature>` 2. Make a tiny change; verify locally 3. `git add ... && git commit -m \"<scope>: <message>\"` 4. `git push -u origin <feature>` ‚Üí Open PR ‚Üí Merge ‚Üí Delete branch 5. Sync: `git checkout main && git pull` --- ## üß™ Verification Cheatsheet ```bash # Stats python scripts/ingest.py # Chunking python scripts/chunk.py # Index build python scripts/index_jsonl.py && wc -l data/index/policies.jsonl # CLI search python scripts/search_jsonl.py \"pto accrual\" --topk 3 # API search curl \"http://127.0.0.1:8000/search?q=pto%20accrual&topk=3\" ``` --- ## üß≠ Roadmap (Phases 3‚Äì6) **Phase 3 ‚Äî Embeddings & Vector Search** * Choose embeddings: local (`sentence-transformers`) vs API provider * `scripts/embed_index.py` ‚Üí create vectors (e.g., `.npy` + `meta.json`) * `scripts/vector_search.py` ‚Üí cosine similarity top‚Äêk * Extend Flask `/search?mode=vector` with citations (doc_id + chunk_id) **Phase 4 ‚Äî Web UI** * Minimal search page calling `/search` * Display sources + highlighted snippets **Phase 5 ‚Äî Deployment & CI/CD** * Deploy on Render/Railway (free tier) * GH Actions: deploy on `main` **Phase 6 ‚Äî Evaluation** * 15‚Äì30 Q/A set over policies * Metrics: groundedness, citation accuracy, latency (p50/p95) * Report in `design-and-evaluation.md` --- ## üõ†Ô∏è Troubleshooting * `ModuleNotFoundError: flask` ‚Üí `source .venv/bin/activate` * 404 on `/search` ‚Üí ensure route is defined **before** `app.run(...)`; restart server * `curl` to `app.github.dev` shows nothing ‚Üí test via `http://127.0.0.1:8000` inside terminal --- ## ü§ñ AI Use & CI Disclosure * AI helpers: **ChatGPT‚Äê5** (mentor/co‚Äêdev) + optional **GitHub Copilot** * CI: runs on each push/PR, installs deps, smoke‚Äêtests `import app` --- **Maintainer:** Aryan Yaghobi **Mentor / AI Co‚ÄêDeveloper:** ChatGPT‚Äê5 > This README documents Phase 2 completion and provides clear run steps, verification, and a rubric‚Äêaligned roadmap.",
    "vector_index": 2
  },
  {
    "doc_id": "readme",
    "chunk_id": 1,
    "source_file": "readme.jsonl",
    "method": "token",
    "params": {
      "window_size": 700,
      "overlap": 150,
      "tokenization": "whitespace_approx"
    },
    "source_hash": "ded7e1292b1c3132e57d9bc201765cec81a4fe4801703ce8116d37917026c58e",
    "text": "**Phase 4 ‚Äî Web UI** * Minimal search page calling `/search` * Display sources + highlighted snippets **Phase 5 ‚Äî Deployment & CI/CD** * Deploy on Render/Railway (free tier) * GH Actions: deploy on `main` **Phase 6 ‚Äî Evaluation** * 15‚Äì30 Q/A set over policies * Metrics: groundedness, citation accuracy, latency (p50/p95) * Report in `design-and-evaluation.md` --- ## üõ†Ô∏è Troubleshooting * `ModuleNotFoundError: flask` ‚Üí `source .venv/bin/activate` * 404 on `/search` ‚Üí ensure route is defined **before** `app.run(...)`; restart server * `curl` to `app.github.dev` shows nothing ‚Üí test via `http://127.0.0.1:8000` inside terminal --- ## ü§ñ AI Use & CI Disclosure * AI helpers: **ChatGPT‚Äê5** (mentor/co‚Äêdev) + optional **GitHub Copilot** * CI: runs on each push/PR, installs deps, smoke‚Äêtests `import app` --- **Maintainer:** Aryan Yaghobi **Mentor / AI Co‚ÄêDeveloper:** ChatGPT‚Äê5 > This README documents Phase 2 completion and provides clear run steps, verification, and a rubric‚Äêaligned roadmap.",
    "vector_index": 3
  },
  {
    "doc_id": "rubric",
    "chunk_id": 0,
    "source_file": "rubric.jsonl",
    "method": "token",
    "params": {
      "window_size": 700,
      "overlap": 150,
      "tokenization": "whitespace_approx"
    },
    "source_hash": "d9c15a8c01cabcec18e8e3e897328727bcc27c36f411ea30058841bee87dcc54",
    "text": "AI Engineering Project Project Overview For this project, you will be designing, building, and evaluating a Retrieval-Augmented Generation (RAG) LLM-based application that answers user questions about a corpus of company policies & procedures. You will then deploy the application to a free-tier host (e.g., Render, Railway) with a basic CI/CD pipeline (e.g., GitHub Actions) that triggers deployment on push/PR when the app builds successfully. Finally, you will demonstrate the system via a screen-share video showing key features of your deployed application, and a quick walkthrough of your design, evaluation and CI/CD run. You can complete this project either individually or as a group of no more than three people. While you can fully hand code this project if you wish, you are highly encouraged to utilize leading AI code generation models/AI IDEs/async agents to assist in rapidly producing your solution, being sure to describe in broad terms how you made use of them. Here are some examples of very useful AI tools you may wish to consider. You will be graded on the quality and functionality of the application and how well it meets the project requirements‚Äîno given proportion of the code is required to be hand coded. Learning Outcomes When completed successfully, this project will enable you to: ‚óè Demonstrate excellent AI engineering skills ‚óè Demonstrate the ability to select appropriate AI application design and architecture Implement a working LLM-based application including RAG ‚óè ‚óè Evaluate the performance of an LLM-based application ‚óè Utilize AI tooling as appropriate ¬© 2025 Quantic Holdings, Inc. All rights reserved. 6/23/21 AI Engineering Project Project Description First, assemble a small but coherent corpus of documents outlining company policies & procedures - about 5‚Äì20 short markdown/HTML/PDF/TXT files totaling 30‚Äì120 pages. You may author them yourself (with AI assistance) or use policies that you are aware of from your own organization that can be used for this assignment. Students must use a corpus they can legally include in the repo or load at runtime (e.g., your own synthetic policies, your organization‚Äôs employee policy documents etc.)‚Äîno private/paid data is required. Additionally, you should define success metrics for your application (see the ‚ÄúEvaluation‚Äù step below), including at least one information-quality metric (e.g., groundedness or citation accuracy) and one system metric (e.g., latency). Use free or zero-cost options when possible e.g., OpenRouter‚Äôs free tier (https://openrouter.ai/docs/api-reference/limits), Groq (https://console.groq.com/docs/rate-limits), or your own paid API keys if you have them. For embedding models, free-tier options are available from Cohere, Voyage, HuggingFace and others Complete the following steps to fully develop, deploy, and evaluate your application: 1. Environment and Reproducibility ‚óã Create a virtual environment (e.g., venv, conda). ‚óã List dependencies in requirements.txt (or environment.yml). ‚óã Provide a README.md with setup + run instructions. ‚óã Set fixed seeds where/if applicable (for deterministic chunking or evaluation sampling). 2. Ingestion and Indexing ‚óã Parse & clean documents (handle PDFs/HTML/md/txt). ‚óã Chunk documents (e.g., by headings or token windows with overlap). ‚óã Embed chunks with a free embedding model or a free-tier API. ‚óã Store the embedded document chunks in a local or lightweight vector database (e.g. Chroma or optionally a cloud-hosted vector store like Pinecone, etc.) ‚óã Store vectors in a local/vector DB or cloud DB (e.g., Chroma, Pinecone, etc.) 3. Retrieval and Generation (RAG) ‚óã To build your RAG pipeline you may use frameworks such as LangChain to handle retrieval, prompt chaining, and API calls, or implement these manually. Implement Top-k retrieval with optional re-ranking. ‚óã ¬© 2025 Quantic Holdings, Inc. All rights reserved. 6/23/21 2 AI Engineering Project ‚óã Build a prompting strategy that injects retrieved chunks (and citations/sources) into the LLM context. ‚óã Add basic guardrails: ‚ñ† Refuse to answer outside the corpus (‚ÄúI can only answer about our policies‚Äù), ‚ñ† Limit output length, ‚ñ† Always cite source doc IDs/titles for answers. 4. Web Application ‚óã Students can use Flask, Streamlit or alternative for the Web app. LangChain is recommended for orchestration, but is optional. ‚óã Endpoints/UI: ‚ñ† ‚ñ† / - Web chat interface - text box for user input /chat - API endpoint that receives user questions (POST) and returns model-generated answers with citations and snippets (link to source and show snippet). /health - returns simple status via JSON. ‚ñ†",
    "vector_index": 4
  },
  {
    "doc_id": "rubric",
    "chunk_id": 1,
    "source_file": "rubric.jsonl",
    "method": "token",
    "params": {
      "window_size": 700,
      "overlap": 150,
      "tokenization": "whitespace_approx"
    },
    "source_hash": "d9c15a8c01cabcec18e8e3e897328727bcc27c36f411ea30058841bee87dcc54",
    "text": "frameworks such as LangChain to handle retrieval, prompt chaining, and API calls, or implement these manually. Implement Top-k retrieval with optional re-ranking. ‚óã ¬© 2025 Quantic Holdings, Inc. All rights reserved. 6/23/21 2 AI Engineering Project ‚óã Build a prompting strategy that injects retrieved chunks (and citations/sources) into the LLM context. ‚óã Add basic guardrails: ‚ñ† Refuse to answer outside the corpus (‚ÄúI can only answer about our policies‚Äù), ‚ñ† Limit output length, ‚ñ† Always cite source doc IDs/titles for answers. 4. Web Application ‚óã Students can use Flask, Streamlit or alternative for the Web app. LangChain is recommended for orchestration, but is optional. ‚óã Endpoints/UI: ‚ñ† ‚ñ† / - Web chat interface - text box for user input /chat - API endpoint that receives user questions (POST) and returns model-generated answers with citations and snippets (link to source and show snippet). /health - returns simple status via JSON. ‚ñ† 5. Deployment ‚óã For production hosting use Render or Railway free tiers; students may alternatively use any other free-tier providers of their choice. ‚óã Configure environment variables (e.g. API keys, model endpoints, DB related etc.). ‚óã Ensure the app is publicly accessible at a shareable URL. 6. CI/CD ‚óã Minimal automated testing is sufficient for this assignment (a build/run check, optional smoke test). ‚óã Create a GitHub Actions workflow that on push/PR: Installs dependencies, ‚ñ† ‚ñ† Runs a build/start check (e.g., python -m pip install -r requirements.txt and python -c \"import app\" or pytest -q if you add tests), ‚ñ† On success in main, deploy to your host (Render/Railway action or via webhook/API). 7. Evaluation of the LLM Application ‚óã Provide a small evaluation set of 15‚Äì30 questions covering various policy topics (PTO, security, expense, remote work, holidays, etc.). Report: ‚ñ† Answer Quality (required): 1. Groundedness: % of answers whose content is factually consistent with and fully supported by the retrieved evidence‚Äîi.e., the answer contains no information that is absent or contradicted in the context. ¬© 2025 Quantic Holdings, Inc. All rights reserved. 6/23/21 3 AI Engineering Project 2. Citation Accuracy: % of answers whose listed citations correctly point to the specific passage(s) that support the information stated‚Äîi.e., the attribution is correct and not misleading. 3. Exact/Partial Match (optional): % of answers that exactly or partially match a short gold answer you provide. ‚ñ† System Metrics (required): 1. Latency (p50/p95) from request to answer for 10‚Äì20 queries. ‚ñ† Ablations (optional): compare retrieval k, chunk size, or prompt variants. 8. Design Documentation ‚óã Briefly justify design choices (embedding model, chunking, k, prompt format, vector store). Submission Guidelines Your final submission should consist of two links: ‚óè A link to an accessible software repository (a GitHub repo) containing all your developed code and the items listed below. You must share your repository with the GitHub account, quantic-grader. o The GitHub repository should include a link to the deployed version of your RAG LLM-based application (in file deployed.md) o The GitHub repository must include a README.md file indicating setup and run instructions o The GitHub repository must also include a brief design and evaluation document (design-and-evaluation.md) listing and explaining: i) ii) design and architecture decisions made - and why they were made, including technology choices summary of your evaluation approach and results for your RAG system o The GitHub repository must include an ai-use.md file that briefly describes which AI code tools you used and how. ‚óè A link to a recorded screen-share demonstration video of the working RAG LLM-based application, involving screen capture of it being used with voiceover o All group members must speak and be present on camera. o All group members must show their government ID. o The demonstration/presentation should be between 5 and 10 minutes long. To submit your project, please click on the \"Submit Project\" button on your dashboard and follow the steps provided. If you are submitting your project as a group, please ¬© 2025 Quantic Holdings, Inc. All rights reserved. 6/23/21 4 AI Engineering Project Project Rubric Scores 2 and above are considered passing. Students who receive a 1 or 0 will not get credit for the assignment and must revise and resubmit to receive a passing grade. Score Description 5 ‚óè Addresses ALL of",
    "vector_index": 5
  },
  {
    "doc_id": "rubric",
    "chunk_id": 2,
    "source_file": "rubric.jsonl",
    "method": "token",
    "params": {
      "window_size": 700,
      "overlap": 150,
      "tokenization": "whitespace_approx"
    },
    "source_hash": "d9c15a8c01cabcec18e8e3e897328727bcc27c36f411ea30058841bee87dcc54",
    "text": "ai-use.md file that briefly describes which AI code tools you used and how. ‚óè A link to a recorded screen-share demonstration video of the working RAG LLM-based application, involving screen capture of it being used with voiceover o All group members must speak and be present on camera. o All group members must show their government ID. o The demonstration/presentation should be between 5 and 10 minutes long. To submit your project, please click on the \"Submit Project\" button on your dashboard and follow the steps provided. If you are submitting your project as a group, please ¬© 2025 Quantic Holdings, Inc. All rights reserved. 6/23/21 4 AI Engineering Project Project Rubric Scores 2 and above are considered passing. Students who receive a 1 or 0 will not get credit for the assignment and must revise and resubmit to receive a passing grade. Score Description 5 ‚óè Addresses ALL of the project requirements, but not limited to: ‚óã Outstanding RAG application with correct responses with matching citations, ingest and indexing works ‚óã Excellent, well-structured application architecture ‚óã Public deployment on Render, Railway (or equivalent) fully functional ‚óã CI/CD runs on push/PR and deploys on success ‚óã Excellent documentation of design choices. ‚óã Excellent evaluation results, which includes groundedness, citation accuracy, and latency ‚óã Excellent, clear demo of features, design and evaluation ‚óè Addresses MOST of the project requirements, but not limited to: ‚óã Excellent RAG application with correct responses with generally matching citations, ingest and indexing works ‚óã Very good, well-structured application architecture ‚óã Public deployment on Render, Railway (or equivalent) almost fully functional ‚óã CI/CD runs on push/PR and deploys on success ‚óã Very good documentation of design choices. ‚óã Very good evaluation results which includes groundedness, citation accuracy, and latency ‚óã Very good, clear demo of features, design and evaluation ‚óè Addresses SOME of the project requirements, but not limited to: ‚óã Very good RAG application with mainly correct responses with generally matching citations, ingest and indexing works ‚óã Good, well-structured application architecture ‚óã Public deployment on Render, Railway (or equivalent) almost fully functional ‚óã CI/CD runs on push/PR and deploys on success ‚óã Good documentation of design choices. 4 3 ¬© 2025 Quantic Holdings, Inc. All rights reserved. 6/23/21 6 AI Engineering Project ‚óã Good evaluation results which includes most of groundedness, citation accuracy, and latency ‚óã Good, clear demo of features, design and evaluation. ‚óè Addresses FEW of the project requirements, but not limited to: ‚óã Passable RAG application with limited correct responses with few matching citations, ingest and indexing works partially ‚óã Passable application architecture ‚óã Public deployment on Render, Railway (or equivalent) not fully functional ‚óã CI/CD runs on push/PR and deploys on success ‚óã Passable documentation of design choices. ‚óã Passable evaluation results which includes only some of groundedness, citation accuracy, and latency ‚óã Passable demo of features, design and evaluation ‚óè Addresses the project but MOST of the project requirements are missing, but not limited to: Incomplete app; not deployed, ‚óã ‚óã No CI/CD, ‚óã No to very limited evaluation ‚óã No design documentation ‚óã No demo of application ‚óè The student either did not complete the assignment, plagiarized all or part of the assignment, or completely failed to address the project requirements. 2 1 0 ¬© 2025 Quantic Holdings, Inc. All rights reserved. 6/23/21 7",
    "vector_index": 6
  },
  {
    "doc_id": "rubric",
    "chunk_id": 3,
    "source_file": "rubric.jsonl",
    "method": "token",
    "params": {
      "window_size": 700,
      "overlap": 150,
      "tokenization": "whitespace_approx"
    },
    "source_hash": "d9c15a8c01cabcec18e8e3e897328727bcc27c36f411ea30058841bee87dcc54",
    "text": "rights reserved. 6/23/21 7",
    "vector_index": 7
  },
  {
    "doc_id": "subtitle_1",
    "chunk_id": 0,
    "source_file": "subtitle_1.jsonl",
    "method": "token",
    "params": {
      "window_size": 700,
      "overlap": 150,
      "tokenization": "whitespace_approx"
    },
    "source_hash": "92c804057c9f9d2d9c604104286739bf06c07e1cee3c28de26195fefcfe93fd9",
    "text": "Let's travel back through time to the 1960s, when the seeds of modern design thinking were planted. This era, known for its dynamic social changes and technological advances, also marked the birth of a new approach to design. As we explore the origins and evolution of design thinking, we'll uncover its deep-rooted connection to solving complex, multifaceted issues known as \"wicked problems\", with a special focus on the contributions of UC Berkeley in shaping this narrative. By the end of this video, you should be able to define design science and explain its significance in the evolution of design thinking, describe the characteristics of \"wicked problems\", and articulate why these problems defy traditional problem-solving methods. After the Industrial Revolution and World War II, the world underwent major shifts. Art, societal changes, technology, human and environmental needs, even philosophical questions all made it clear that we were facing increasingly complex problems. Initially emerging in the 1950s and 1960s within architecture and engineering, the concept of design thinking as it was then struggled to adapt to the era's rapid changes. The scope and scale and complexity of strategic thinking needed in World War II fundamentally changed the way we approach management, production, and industrial design in the modern world. In the 1960s, attempts to apply scientific methods to design aimed at making it a rigorous discipline. Around this time, Herbert Simon introduced the world to \"design science\". Simon was a pioneering figure in numerous fields, including artificial intelligence, psychology, computer science, and management science, known for his profound contributions to understanding of human decision-making processes and problem-solving. Simon proposed design not just as an art but as a systematic, scientific discipline. This groundbreaking idea, detailed in The Sciences of the Artificial, suggested that design could solve not only just practical problems, but also abstract, complex challenges by creating innovative solutions. This concept laid the foundation for design thinking, emphasizing a structured, methodical approach to uncovering and solving problems. The 1970s saw Herb Simon and others further conceptualize design as a way of thinking, emphasizing rapid prototyping and testing, laying down principles that are central to design thinking today. This era also marked advancements in artificial intelligence and its implications for design. The 1980s recognized solution-focused problem-solving, with Nigel Cross and Brian Lawson comparing the problem-solving approaches of designers and scientists, highlighting designers' preference for generating multiple solutions, known as satisficing behavior. Nigel Cross, now Emeritus Professor of Design Studies at The Open University in the UK, thoroughly examined what makes design its own special field of study. In 1982, Cross wrote a collection of papers called Designerly Ways of Knowing. These papers are foundational in the history of design. I encourage you to read them to learn more. One important takeaway from this work was that when it comes down to it, design isn't just about making things look good, it's really about problem-solving, and thinking, in a way that's different from how scientists or scholars tackle problems. It's like design has its own language and tools for solving puzzles that you won't find in other fields. As Cross describes it, \"A central feature of design activity, then, is its reliance on generating fairly quickly a satisfactory solution, rather than on any prolonged analysis of the problem. It is a process of 'satisficing', rather than optimizing, producing any one of what might well be a large range of satisfactory solutions, rather than attempting to generate the one hypothetically optimum solution. This strategy has been observed in other studies of design behavior, including architects, urban designers, and engineers\". This thread of research continued and was expanded by many. Brian Lawson, a professor from the University of Sheffield, was responsible for some fascinating experiments comparing how architecture students, or designers, and science students differed in their approach to solving problems. Lawson had designers arrange three-dimensional colored blocks following certain known and unknown rules, the most interesting takeaway being the following. While the scientists took a step-by-step approach to figure out the rules, designers were all about trying out different solutions until they identified one that worked. This demonstrated that designers are more likely to seek a workable solution right away, rather than dissecting the",
    "vector_index": 8
  },
  {
    "doc_id": "subtitle_1",
    "chunk_id": 1,
    "source_file": "subtitle_1.jsonl",
    "method": "token",
    "params": {
      "window_size": 700,
      "overlap": 150,
      "tokenization": "whitespace_approx"
    },
    "source_hash": "92c804057c9f9d2d9c604104286739bf06c07e1cee3c28de26195fefcfe93fd9",
    "text": "'satisficing', rather than optimizing, producing any one of what might well be a large range of satisfactory solutions, rather than attempting to generate the one hypothetically optimum solution. This strategy has been observed in other studies of design behavior, including architects, urban designers, and engineers\". This thread of research continued and was expanded by many. Brian Lawson, a professor from the University of Sheffield, was responsible for some fascinating experiments comparing how architecture students, or designers, and science students differed in their approach to solving problems. Lawson had designers arrange three-dimensional colored blocks following certain known and unknown rules, the most interesting takeaway being the following. While the scientists took a step-by-step approach to figure out the rules, designers were all about trying out different solutions until they identified one that worked. This demonstrated that designers are more likely to seek a workable solution right away, rather than dissecting the problem bit by bit. As Lawson described it, scientists were problem-focused problem solvers, compared to designers, who were solution-focused problem solvers. This important distinction further encouraged others to study how a repeatable process for solving problems was a hallmark of design thinking. One important contribution came from Peter Rowe's 1987 book, Design Thinking, in which he further explored the design process, specifically in architectural contexts, contributing to the broader understanding of design thinking. As he describes it, it was, quote, \"an attempt to fashion a generalized portrait of design thinking.\" From the 1990s to present, design thinking has been popularized by organizations such as IDEO and many academic institutions across the world. Universities such as Berkeley, Stanford, and Carnegie Mellon, among many others, began making repeatable design methodologies more accessible and widely adopted across various fields. Part of the appeal is a standard toolkit for addressing some of the most difficult and pressing societal challenges, or \"wicked problems\". The ability of design to address our most challenging problems has always been an important thread throughout the history of design. While historically, this concept has often been referred to as \"wicked problems\", you may also be familiar with the term \"complex sociotechnical systems\" that many people prefer to use today. Now let's examine a bit about the history of \"wicked problems\". I personally love this history because of its deep connection to UC Berkeley. The term \"wicked problems\" was actually believed to be coined by design theorist Professor Horst Rittel at UC Berkeley in the 1960s, introducing a new dimension to the challenges design thinking aimed to address. \"Wicked problems\" demand that we think differently, collaborate more broadly, and remain flexible in our approaches. \"Wicked problems\" are characterized by their complexity, resistance to straightforward solutions, and lack of clear definitions. They are large-scale societal, environmental, and organizational challenges like climate change, healthcare, and urban planning, where the problem evolves even as solutions are attempted. Addressing \"wicked problems\" requires an adaptive, interdisciplinary approach focusing on understanding the human element at their core. Let's define \"wicked problems\" broadly. They have no definitive formulation. Solutions are not true or false, but better or worse. Every \"wicked problem\" is essentially unique and novel, and there is no opportunity to learn by trial and error. UC Berkeley's environment has fostered a profound and deep exploration of \"wicked problems\", led by Rittle's early investigations. The university became a place where the concept of collaborative, interdisciplinary problem-solving was nurtured. This approach to education and research emphasized the importance of diverse perspectives and collective brainstorming in crafting innovative solutions to complex issues, a history that we try to uphold today. As we conclude our brief exploration of design thinking's history and its crucial role in solving \"wicked problems\", we're reminded of the power of human-centered design. It's a mindset that looks beyond the surface, diving into the complex web of human needs and societal challenges, armed with empathy, creativity, and the collaborative spirit. The journey from the 1960s to today shows us that design thinking is not just about creating. It's about understanding, adapting, and innovating in the face of uncertainty. As we continue to face new challenges, the principles of design thinking remind us that every problem, no matter how wicked, holds the potential for innovative solutions.",
    "vector_index": 9
  },
  {
    "doc_id": "subtitle_1",
    "chunk_id": 2,
    "source_file": "subtitle_1.jsonl",
    "method": "token",
    "params": {
      "window_size": 700,
      "overlap": 150,
      "tokenization": "whitespace_approx"
    },
    "source_hash": "92c804057c9f9d2d9c604104286739bf06c07e1cee3c28de26195fefcfe93fd9",
    "text": "a place where the concept of collaborative, interdisciplinary problem-solving was nurtured. This approach to education and research emphasized the importance of diverse perspectives and collective brainstorming in crafting innovative solutions to complex issues, a history that we try to uphold today. As we conclude our brief exploration of design thinking's history and its crucial role in solving \"wicked problems\", we're reminded of the power of human-centered design. It's a mindset that looks beyond the surface, diving into the complex web of human needs and societal challenges, armed with empathy, creativity, and the collaborative spirit. The journey from the 1960s to today shows us that design thinking is not just about creating. It's about understanding, adapting, and innovating in the face of uncertainty. As we continue to face new challenges, the principles of design thinking remind us that every problem, no matter how wicked, holds the potential for innovative solutions.",
    "vector_index": 10
  },
  {
    "doc_id": "tasks_now",
    "chunk_id": 0,
    "source_file": "tasks_now.jsonl",
    "method": "token",
    "params": {
      "window_size": 700,
      "overlap": 150,
      "tokenization": "whitespace_approx"
    },
    "source_hash": "2a7920b19db121d41841d30683471838b1b9b1e56a5484b8ff7132a01f64b4ba",
    "text": "Farokhi Email,s Letters, Car Insurance cigo, BrainScape, Credit Crads, Finance WES Sogol Docs Resume 3 letters Applications Dars CBTS 3 licences Quantic Ryan Khan Academy",
    "vector_index": 11
  }
]